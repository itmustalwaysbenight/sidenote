---
layout: post
title: "CS224N Lecture 1"
date: 2024-09-12
external_url: "https://youtu.be/rmVRLeJRkl4?si=KtSKnzO4mDoXTat8"
---



# Summary of Stanford's CS224N: Natural Language Processing with Deep Learning - Lecture 1

I watched and am summarizing the first lecture of Stanford's CS224N, taught by Christopher Manning. This class introduces deep learning applications for Natural Language Processing (NLP), starting with a focus on word meaning and the widely-used word2vec algorithm.

## Course Overview

Christopher Manning introduces the three main goals of the course:

1. **Understanding Modern Deep Learning Methods**: The class will cover deep learning techniques, including recurrent neural networks (RNNs), attention mechanisms, and transformers, all applied to NLP.
2. **Big Picture of Human Language**: Despite the complexities of human language, this course will cover linguistic aspects that are relevant to NLP.
3. **Hands-on PyTorch Development**: Students will learn to build systems for key NLP problems like machine translation, question answering, and learning word meanings.

## Word Meaning and Word Vectors

### The Surprising Power of Word Vectors

Manning highlights how word meanings can be represented as vectors of real numbers. While this is a relatively recent discovery in the last decade of deep learning, it has revolutionized NLP. Word vectors capture the contextual meaning of words, a concept that challenges traditional linguistic views.

### Word2vec Algorithm

Word2vec, developed by Tomas Mikolov in 2013, is a framework for learning word vectors from large corpora of text. The basic idea is to predict context words given a center word using distributional semantics. The key assumption here is the famous quote from J.R. Firth: "You shall know a word by the company it keeps."

The algorithm's objective is to maximize the likelihood of correctly predicting context words. Formally, the objective function is the log likelihood of the context words, given the center word:

$$
J(\theta) = - \frac{1}{T} \sum_{t=1}^{T} \sum_{j=-M}^{M} \log P(w_{t+j} | w_t; \theta)
$$

Where \( P(w_{t+j} | w_t; \theta) \) is the probability of context word \( w_{t+j} \) given the center word \( w_t \), and \( M \) is the window size.

### Softmax and Optimization

The model uses the softmax function to normalize the dot product of the center and context word vectors, ensuring it outputs a valid probability distribution:

$$
P(w_O | w_C) = \frac{\exp(v_O \cdot v_C)}{\sum_{W} \exp(v_W \cdot v_C)}
$$

To optimize this model, word vectors are randomly initialized, and gradients of the objective function are calculated via the chain rule, adjusting the vectors iteratively. The optimization seeks to minimize the loss and maximize the model's ability to predict context words.

### Embeddings and Vector Space

Word embeddings place words into a high-dimensional vector space. For example, in a 300-dimensional space, similar words like "king" and "queen" will be close together, forming clusters of related words (e.g., countries, verbs). Importantly, word vectors allow for analogical reasoning, such as:

$$
\text{king} - \text{man} + \text{woman} \approx \text{queen}
$$

These embeddings are learned from raw text data, which makes them powerful tools for understanding and processing language.

## Challenges and Future Topics

While word2vec works surprisingly well, there are limitations, such as its inability to handle antonyms and capture nuanced meanings like sentiment. Additionally, words with multiple meanings (e.g., "star" as both an astronomical object and a celebrity) are represented by a single vector, which can be problematic.



