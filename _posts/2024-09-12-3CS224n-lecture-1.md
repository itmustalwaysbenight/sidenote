---
layout: post
title: "CS224N Lecture 1"
date: 2024-09-12
external_url: "https://youtu.be/rmVRLeJRkl4?si=KtSKnzO4mDoXTat8"
---


# Summary of Video Lecture: NLP with Deep Learning | Winter 2021 | Lecture 1 - Intro & Word Vectors

I watched and am summarizing a lecture given by Christopher Manning on natural language processing (NLP) with deep learning. This summary reflects my personal learning process, focusing on word vectors and distributional semantics.

## Goals of the Course
The course has three main objectives:
1. Understanding deep learning methods applied to NLP.
2. Exploring the complexities of human language and its relation to NLP.
3. Gaining hands-on experience building NLP systems in PyTorch, with a focus on tasks like word meaning learning, machine translation, and question answering.

## Introduction to Human Language and Word Meaning
Human language is a social system that evolves with its users, making it challenging for computers to process. Unlike formal systems like programming languages, human language is fluid and full of ambiguities. These characteristics drive the difficulty of natural language understanding in computational systems.

### Distributional Semantics and Word2Vec
One of the key concepts introduced is distributional semantics, the idea that a word's meaning is determined by the company it keeps, as famously stated by J.R. Firth. In NLP, this leads to the modern practice of encoding word meanings using vectors. Specifically, the **word2vec** algorithm was introduced as a method to learn these vector representations, or **word embeddings**.

In word2vec, word vectors are learned by predicting the surrounding context words given a center word. The process involves calculating the probability of context words and adjusting word vectors to improve these predictions iteratively.

### Mathematical Foundations
Word2Vec employs the following key concepts:
- **Word vectors**: Each word has a vector representation in a high-dimensional space (often 300 dimensions).
- **Objective function**: The objective is to minimize the negative log-likelihood of the context words, formalized as:
  \[
  J(\theta) = - \frac{1}{T} \sum_{t=1}^T \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j} \mid w_t)
  \]
  Here, \( w_t \) represents the center word, and \( w_{t+j} \) are the context words within a window of size \( m \).

- **Softmax function**: To calculate the probability of a context word \( w_o \) given a center word \( w_c \), word2vec uses the softmax function:
  \[
  P(w_o \mid w_c) = \frac{e^{v_o^T v_c}}{\sum_{w=1}^{V} e^{v_w^T v_c}}
  \]
  where \( v_o \) and \( v_c \) are the word vectors for the context word and the center word, respectively, and \( V \) is the vocabulary size.

### Gradients and Optimization
To improve word vectors, we calculate the gradients of the objective function with respect to the vectors, using multivariate calculus:
\[
\frac{\partial J(\theta)}{\partial v_c} = v_o - \sum_{w=1}^{V} P(w \mid w_c) v_w
\]
The word vectors are updated by moving in the direction of the negative gradient, ensuring that the model better predicts the actual context words.

### Word Embeddings and Semantic Relationships
Word embeddings capture relationships between words through vector arithmetic. For example, vector operations like \( \text{king} - \text{man} + \text{woman} \) often result in vectors close to **queen**. This reveals the ability of word2vec to capture both syntactic and semantic relationships.

### Practical Application: Similarity and Analogies
Word2Vec not only groups similar words together in the vector space (e.g., **USA** and **Canada**), but it also enables analogy-based reasoning, where vector arithmetic reflects linguistic analogies, such as:
\[
\text{man} : \text{king} :: \text{woman} : \text{queen}
\]
This capability allows NLP systems to perform more human-like reasoning over word meanings.

## Conclusion
This lecture introduced the foundations of NLP with deep learning, particularly word2vec and word embeddings. These methods provide a powerful way to represent word meaning in high-dimensional space, forming the basis for many modern NLP systems.
