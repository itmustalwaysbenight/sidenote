---
layout: post
title: "CS224N Lecture 1"
date: 2024-09-12
external_url: "https://youtu.be/rmVRLeJRkl4?si=KtSKnzO4mDoXTat8"
---

# Summary of "NLP with Deep Learning | Winter 2021 | Lecture 1 - Intro & Word Vectors"

I watched and am summarizing the video content of Stanford's CS224N, where Christopher Manning introduces core concepts in Natural Language Processing (NLP) with Deep Learning, focusing on word vectors and the word2vec algorithm.

## Introduction to Word Vectors

The lecture begins by addressing how human language can be complex for computers to understand due to its social nature. Manning emphasizes that word vectors allow us to represent word meaning as a high-dimensional vector, which helps models predict the context in which a word appears. This result is particularly surprising because it defies thousands of years of traditional linguistic theory.

Word vectors are real-numbered vectors that represent words in a continuous vector space. Similar words have similar vectors, allowing for operations like analogies to be performed (e.g., \\( \text{king} - \text{man} + \text{woman} \approx \text{queen} \\)). This provides a powerful way to measure semantic similarity.

## Human Language and Machine Translation

Human language is an evolving system, constructed and interpreted by people. Its inherent complexity makes it difficult for machines to process. Despite this, recent progress in NLP, especially in machine translation, has led to tools like Google Translate, which can moderately translate languages such as Swahili to English with reasonable accuracy.

The most significant development in NLP has been models like GPT-3, which can perform a variety of tasks using a single, large model trained on vast amounts of data. The key capability of GPT-3 is its ability to predict the next word in a sequence based on its context, enabling it to complete various tasks without the need for task-specific training.

## Distributional Semantics

One of the fundamental ideas in NLP is distributional semantics, which captures word meaning based on the words that frequently appear near it. This concept is encapsulated in J.R. Firth's famous quote, "You shall know a word by the company it keeps." The distributional hypothesis forms the basis of word vector models.

Words are represented by vectors, which are updated through an optimization process to predict the context words around them. For example, in the sentence "The cat sat on the mat," "sat" could be the center word, and its surrounding words ("The", "cat", "on", "the", "mat") are its context words.

## Word2Vec Algorithm

The word2vec algorithm, introduced by Tomas Mikolov and colleagues, provides an efficient way to learn word vectors from large text corpora. It uses two vectors per word: one when the word is the "center word" (\\( v_C \\)) and one when it is a "context word" (\\( u_O \\)).

The algorithm calculates the probability of context words given a center word, using the softmax function:

\\[
P(O | C) = \frac{\exp(u_O^T v_C)}{\sum_{w \in V} \exp(u_w^T v_C)}
\\]

To minimize the loss, word vectors are adjusted through gradient descent. The goal is to maximize the likelihood of predicting the actual context words that appear around each center word.

## Gradient Descent and Optimization

Gradient descent is used to update the word vectors by computing the gradients of the loss function with respect to the word vectors. This involves calculating the partial derivatives of the softmax function to determine how to adjust the vectors to increase the likelihood of predicting the correct context words.

For each center word and context word, the gradient of the objective function is calculated and used to update the vectors. After many iterations, the word vectors become good representations of word meanings.

## Practical Use of Word Vectors

Word vectors have many practical applications. They can be used to find similar words, perform analogies, and even model semantic relationships between words. For instance, word vectors allow us to solve analogy tasks such as:

\\[
\text{king} - \text{man} + \text{woman} \approx \text{queen}
\\]

Similarly, word vectors can capture relationships like:

\\[
\text{Australia} : \text{beer} :: \text{France} : \text{champagne}
\\]

In conclusion, word vectors provide a powerful tool for representing meaning in NLP. They allow us to encode semantic relationships between words in a continuous space, facilitating tasks like translation, information retrieval, and sentiment analysis.
